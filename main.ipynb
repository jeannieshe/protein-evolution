{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cf4f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kspiv/.conda/envs/rl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from fitness_functions import fitness_ESM, fitness_ESM_DMS\n",
    "from callbacks import TQDMCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350de72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    State: amino acid sequence (string or int array)\n",
    "    Action: mutate position i to amino acid j\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, seq, fitness_fn, DMS_path):\n",
    "        ''' Requires the wild-type aa sequence (string), \n",
    "                fitness_fn (defined in fitness_functions.py),\n",
    "            and DMS dataset (path to csv)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        self.aa_to_idx = {aa: i for i, aa in enumerate(self.amino_acids)}\n",
    "        self.idx_to_aa = {i: aa for aa, i in self.aa_to_idx.items()}\n",
    "\n",
    "        self.L = len(seq)\n",
    "        self.fitness_fn = fitness_fn\n",
    "        self.DMS = pd.read_csv(DMS_path)\n",
    "        \n",
    "        # convert sequence string â†’ array of indices\n",
    "        self.initial_seq = np.array([self.aa_to_idx[a] for a in seq], dtype=np.int32)\n",
    "\n",
    "        # action = choose a position to mutate, and choose an aa to mutate to\n",
    "        self.action_space = spaces.Discrete(self.L * 20)\n",
    "\n",
    "        # observation = vector of length L with values in [0,19]\n",
    "        self.observation_space = spaces.MultiDiscrete([20] * self.L)\n",
    "\n",
    "        self.state = None\n",
    "    \n",
    "    def idxs_to_letters(self, seq):\n",
    "        ''' convert string of indexes to string of aa letters '''\n",
    "        return ''.join([self.idx_to_aa[i] for i in seq])\n",
    "\n",
    "    def _decode_action(self, action):\n",
    "        pos = action // 20\n",
    "        aa_idx = action % 20\n",
    "        return pos, aa_idx\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = self.initial_seq.copy()  # back to wild-type\n",
    "        obs = self.state.copy()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        import pdb;pdb.set_trace()\n",
    "        pos, aa_idx = self._decode_action(action)\n",
    "\n",
    "        # Apply mutation\n",
    "        new_state = self.state.copy()\n",
    "        new_state[pos] = aa_idx\n",
    "\n",
    "        # Reward from fitness function\n",
    "        reward = self.fitness_fn(self.idxs_to_letters(new_state), self.DMS)\n",
    "\n",
    "        # You can choose episode termination rule:\n",
    "        # e.g., fixed length episode of mutations\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        self.state = new_state\n",
    "        import pdb;pdb.set_trace()\n",
    "        return new_state.copy(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        seq_str = \"\".join(self.idx_to_aa[i] for i in self.state)\n",
    "        print(seq_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaff53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, esm\n",
    "import numpy as np\n",
    "\n",
    "# Load the pretrained ESM2 150M model\n",
    "esm150_model, esm150_alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "esm150_model.eval()\n",
    "\n",
    "batch_converter = esm150_alphabet.get_batch_converter()\n",
    "mask_idx = esm150_alphabet.mask_idx\n",
    "def esm_pseudo_log_likelihood(seq):\n",
    "    \"\"\"\n",
    "    Computes the pseudo log-likelihood of an amino-acid sequence using ESM2 (masked LM). \n",
    "    Returns a float.\n",
    "    \"\"\"\n",
    "    data = [(\"protein\", seq)]\n",
    "    import pdb;pdb.set_trace()\n",
    "    _, _, tokens = batch_converter(data)     # shape [1, L]\n",
    "    tokens = tokens[0]                       # shape [L] (technically 2 more then length bc of BOS/EOS tokens)\n",
    "    L = tokens.size(0)\n",
    "\n",
    "    # Generate all masked sequences (L-2 internal positions)\n",
    "    masked_tokens = tokens.repeat(L-2, 1)\n",
    "    positions = torch.arange(1, L-1)\n",
    "    masked_tokens[torch.arange(L-2), positions] = mask_idx  # mask each pos\n",
    "\n",
    "    # Add batch dimension\n",
    "    masked_tokens = masked_tokens.unsqueeze(1)  # [L-2, 1, L]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        import pdb;pdb.set_trace()\n",
    "        logits = esm150_model(masked_tokens)[\"logits\"]  # [L-2, 1, L, vocab]\n",
    "\n",
    "    log_probs = []\n",
    "    for i, pos in enumerate(positions):\n",
    "        true_token = tokens[pos]\n",
    "        log_prob_i = torch.log_softmax(logits[i, 0, pos], dim=-1)[true_token]\n",
    "        log_probs.append(log_prob_i)\n",
    "\n",
    "    return float(torch.stack(log_probs).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a04eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m17\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     15\u001b[39m     data = [(\u001b[33m\"protein\"\u001b[39m, seq)]\n",
      "\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb;pdb.set_trace()\n",
      "\u001b[32m---> 17\u001b[39m     _, _, tokens = batch_converter(data)     \u001b[38;5;66;03m# shape [1, L]\u001b[39;00m\n",
      "\u001b[32m     18\u001b[39m     tokens = tokens[\u001b[32m0\u001b[39m]                       \u001b[38;5;66;03m# shape [L] (technically 2 more then length bc of BOS/EOS tokens)\u001b[39;00m\n",
      "\u001b[32m     19\u001b[39m     L = tokens.size(\u001b[32m0\u001b[39m)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m18\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb;pdb.set_trace()\n",
      "\u001b[32m     17\u001b[39m     _, _, tokens = batch_converter(data)     \u001b[38;5;66;03m# shape [1, L]\u001b[39;00m\n",
      "\u001b[32m---> 18\u001b[39m     tokens = tokens[\u001b[32m0\u001b[39m]                       \u001b[38;5;66;03m# shape [L] (technically 2 more then length bc of BOS/EOS tokens)\u001b[39;00m\n",
      "\u001b[32m     19\u001b[39m     L = tokens.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[32m     20\u001b[39m \n",
      "\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m19\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     17\u001b[39m     _, _, tokens = batch_converter(data)     \u001b[38;5;66;03m# shape [1, L]\u001b[39;00m\n",
      "\u001b[32m     18\u001b[39m     tokens = tokens[\u001b[32m0\u001b[39m]                       \u001b[38;5;66;03m# shape [L] (technically 2 more then length bc of BOS/EOS tokens)\u001b[39;00m\n",
      "\u001b[32m---> 19\u001b[39m     L = tokens.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[32m     20\u001b[39m \n",
      "\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Generate all masked sequences (L-2 internal positions)\u001b[39;00m\n",
      "\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m22\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \n",
      "\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Generate all masked sequences (L-2 internal positions)\u001b[39;00m\n",
      "\u001b[32m---> 22\u001b[39m     masked_tokens = tokens.repeat(L-\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[32m     23\u001b[39m     positions = torch.arange(\u001b[32m1\u001b[39m, L-\u001b[32m1\u001b[39m)\n",
      "\u001b[32m     24\u001b[39m     masked_tokens[torch.arange(L-\u001b[32m2\u001b[39m), positions] = mask_idx  \u001b[38;5;66;03m# mask each pos\u001b[39;00m\n",
      "\n",
      "13\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m23\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Generate all masked sequences (L-2 internal positions)\u001b[39;00m\n",
      "\u001b[32m     22\u001b[39m     masked_tokens = tokens.repeat(L-\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[32m---> 23\u001b[39m     positions = torch.arange(\u001b[32m1\u001b[39m, L-\u001b[32m1\u001b[39m)\n",
      "\u001b[32m     24\u001b[39m     masked_tokens[torch.arange(L-\u001b[32m2\u001b[39m), positions] = mask_idx  \u001b[38;5;66;03m# mask each pos\u001b[39;00m\n",
      "\u001b[32m     25\u001b[39m \n",
      "\n",
      "torch.Size([11, 13])\n",
      "32\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m24\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     22\u001b[39m     masked_tokens = tokens.repeat(L-\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[32m     23\u001b[39m     positions = torch.arange(\u001b[32m1\u001b[39m, L-\u001b[32m1\u001b[39m)\n",
      "\u001b[32m---> 24\u001b[39m     masked_tokens[torch.arange(L-\u001b[32m2\u001b[39m), positions] = mask_idx  \u001b[38;5;66;03m# mask each pos\u001b[39;00m\n",
      "\u001b[32m     25\u001b[39m \n",
      "\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m27\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     25\u001b[39m \n",
      "\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "\u001b[32m---> 27\u001b[39m     masked_tokens = masked_tokens.unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [L-2, 1, L]\u001b[39;00m\n",
      "\u001b[32m     28\u001b[39m \n",
      "\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\n",
      "torch.Size([11, 13])\n",
      "> \u001b[32m/tmp/ipykernel_962296/345124178.py\u001b[39m(\u001b[92m29\u001b[39m)\u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     27\u001b[39m     masked_tokens = masked_tokens.unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [L-2, 1, L]\u001b[39;00m\n",
      "\u001b[32m     28\u001b[39m \n",
      "\u001b[32m---> 29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m pdb;pdb.set_trace()\n",
      "\u001b[32m     31\u001b[39m         logits = esm150_model(masked_tokens)[\u001b[33m\"logits\"\u001b[39m]  \u001b[38;5;66;03m# [L-2, 1, L, vocab]\u001b[39;00m\n",
      "\n",
      "torch.Size([11, 1, 13])\n"
     ]
    }
   ],
   "source": [
    "esm_pseudo_log_likelihood('AGRILKYRTTT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054546ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:   0%|          | 0/1 [15:33<?, ?it/s]\n",
      "Training PPO:   0%|          | 0/1 [07:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mesm_pseudo_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMAADGYLPDWLEDTLSEGIRQWWKLKPGPPPPKPAERHKDDSRGLVLPGYKYLGPFNGLDKGEPVNEADAAALEHDKAYDRQLDSGDNPYLKYNHADAEFQERLKEDTSFGGNLGRAVFQAKKRVLEPLGLVEEPVKTAPGKKRPVEHSPVEPDSSSGTGKAGQQPARKRLNFGQTGDADSVPDPQPLGQPPAAPSGLGTNTMATGSGAPMADNNEGADGVGNSSGNWHCDSTWMGDRVITTSTRTWALPTYNNHLYKQISSQSGASNDNHYFGYSTPWGYFDFNRFHCHFSPRDWQRLINNNWGFRPKRLNFKLFNIQVKEVTQNDGTTTIANNLTSTVQVFTDSEYQLPYVLGSAHQGCLPPFPADVFMVPQYGYLTLNNGSQAVGRSSFYCLEYFPSQMLRTGNNFTFSYTFEDVPFHSSYAHSQSLDRLMNPLIDQYLYYLSRTNTPSGTTTQSRLQFSQAGASDIRDQSRNWLPGPCYRQQRVSKTSADNNNSEYSWTGATKYHLNGRDSLVNPGPAMASHKDDEEKFFPQSGVLIFGKQGSEKTNVDIEKVMITDEEEIRTTNPVATEQYGSVSTNLQRGNRQAATADVNTQGVLPGMVWQDRDVYLQGPIWAKIPHTDGHFHPSPLMGGFGLKHPPPQILIKNTPVPANPSTTFSAAKFASFITQYSTGQVSVEIEWELQKENSKRWNPEIQYTSNYNKSVNVDFTVDTNGVYSEPRPIGTRYLTRNL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/weka/scratch/weka/cbmm/kspiv/protein-evolution/fitness_functions.py:47\u001b[39m, in \u001b[36mesm_pseudo_log_likelihood\u001b[39m\u001b[34m(seq)\u001b[39m\n\u001b[32m     44\u001b[39m masked_tokens[\u001b[32m0\u001b[39m, i] = mask_idx\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     logits = \u001b[43mesm150_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_tokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# [1, L, vocab]\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Teacher-forced target token\u001b[39;00m\n\u001b[32m     50\u001b[39m true_token = tokens[\u001b[32m0\u001b[39m, i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/esm/model/esm2.py:112\u001b[39m, in \u001b[36mESM2.forward\u001b[39m\u001b[34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[39m\n\u001b[32m    109\u001b[39m     padding_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x, attn = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx + \u001b[32m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[32m    118\u001b[39m         hidden_representations[layer_idx + \u001b[32m1\u001b[39m] = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/esm/modules.py:125\u001b[39m, in \u001b[36mTransformerLayer.forward\u001b[39m\u001b[34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[39m\n\u001b[32m    123\u001b[39m residual = x\n\u001b[32m    124\u001b[39m x = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(x)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m x, attn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m x = residual + x\n\u001b[32m    136\u001b[39m residual = x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/esm/multihead_attention.py:260\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[39m\n\u001b[32m    258\u001b[39m     q = \u001b[38;5;28mself\u001b[39m.q_proj(query)\n\u001b[32m    259\u001b[39m     k = \u001b[38;5;28mself\u001b[39m.k_proj(key)\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m q *= \u001b[38;5;28mself\u001b[39m.scaling\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "esm_pseudo_log_likelihood('MAADGYLPDWLEDTLSEGIRQWWKLKPGPPPPKPAERHKDDSRGLVLPGYKYLGPFNGLDKGEPVNEADAAALEHDKAYDRQLDSGDNPYLKYNHADAEFQERLKEDTSFGGNLGRAVFQAKKRVLEPLGLVEEPVKTAPGKKRPVEHSPVEPDSSSGTGKAGQQPARKRLNFGQTGDADSVPDPQPLGQPPAAPSGLGTNTMATGSGAPMADNNEGADGVGNSSGNWHCDSTWMGDRVITTSTRTWALPTYNNHLYKQISSQSGASNDNHYFGYSTPWGYFDFNRFHCHFSPRDWQRLINNNWGFRPKRLNFKLFNIQVKEVTQNDGTTTIANNLTSTVQVFTDSEYQLPYVLGSAHQGCLPPFPADVFMVPQYGYLTLNNGSQAVGRSSFYCLEYFPSQMLRTGNNFTFSYTFEDVPFHSSYAHSQSLDRLMNPLIDQYLYYLSRTNTPSGTTTQSRLQFSQAGASDIRDQSRNWLPGPCYRQQRVSKTSADNNNSEYSWTGATKYHLNGRDSLVNPGPAMASHKDDEEKFFPQSGVLIFGKQGSEKTNVDIEKVMITDEEEIRTTNPVATEQYGSVSTNLQRGNRQAATADVNTQGVLPGMVWQDRDVYLQGPIWAKIPHTDGHFHPSPLMGGFGLKHPPPQILIKNTPVPANPSTTFSAAKFASFITQYSTGQVSVEIEWELQKENSKRWNPEIQYTSNYNKSVNVDFTVDTNGVYSEPRPIGTRYLTRNL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3547d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAADGYLPDWLEDTLSEGIRQWWKLKPGPPPPKPAERHKDDSRGLVLPGYKYLGPFNGLDKGEPVNEADAAALEHDKAYDRQLDSGDNPYLKYNHADAEFQERLKEDTSFGGNLGRAVFQAKKRVLEPLGLVEEPVKTAPGKKRPVEHSPVEPDSSSGTGKAGQQPARKRLNFGQTGDADSVPDPQPLGQPPAAPSGLGTNTMATGSGAPMADNNEGADGVGNSSGNWHCDSTWMGDRVITTSTRTWALPTYNNHLYKQISSQSGASNDNHYFGYSTPWGYFDFNRFHCHFSPRDWQRLINNNWGFRPKRLNFKLFNIQVKEVTQNDGTTTIANNLTSTVQVFTDSEYQLPYVLGSAHQGCLPPFPADVFMVPQYGYLTLNNGSQAVGRSSFYCLEYFPSQMLRTGNNFTFSYTFEDVPFHSSYAHSQSLDRLMNPLIDQYLYYLSRTNTPSGTTTQSRLQFSQAGASDIRDQSRNWLPGPCYRQQRVSKTSADNNNSEYSWTGATKYHLNGRDSLVNPGPAMASHKDDEEKFFPQSGVLIFGKQGSEKTNVDIEKVMITDEEEIRTTNPVATEQYGSVSTNLQRGNRQAATADVNTQGVLPGMVWQDRDVYLQGPIWAKIPHTDGHFHPSPLMGGFGLKHPPPQILIKNTPVPANPSTTFSAAKFASFITQYSTGQVSVEIEWELQKENSKRWNPEIQYTSNYNKSVNVDFTVDTNGVYSEPRPIGTRYLTRNL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b34827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_960888/2807009226.py\u001b[39m(\u001b[92m50\u001b[39m)\u001b[36mstep\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m step(self, action):\n",
      "\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m pdb;pdb.set_trace()\n",
      "\u001b[32m---> 50\u001b[39m         pos, aa_idx = self._decode_action(action)\n",
      "\u001b[32m     51\u001b[39m \n",
      "\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Apply mutation\u001b[39;00m\n",
      "\n",
      "10102\n",
      "> \u001b[32m/tmp/ipykernel_960888/2807009226.py\u001b[39m(\u001b[92m53\u001b[39m)\u001b[36mstep\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     51\u001b[39m \n",
      "\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Apply mutation\u001b[39;00m\n",
      "\u001b[32m---> 53\u001b[39m         new_state = self.state.copy()\n",
      "\u001b[32m     54\u001b[39m         new_state[pos] = aa_idx\n",
      "\u001b[32m     55\u001b[39m \n",
      "\n",
      "(np.int64(505), np.int64(2))\n",
      "array([10,  0,  0,  2,  5, 19,  9, 12,  2, 18,  9,  3,  2, 16,  9, 15,  3,\n",
      "        5,  7, 14, 13, 18, 18,  8,  9,  8, 12,  5, 12, 12, 12, 12,  8, 12,\n",
      "        0,  3, 14,  6,  8,  2,  2, 15, 14,  5,  9, 17,  9, 12,  5, 19,  8,\n",
      "       19,  9,  5, 12,  4, 11,  5,  9,  2,  8,  5,  3, 12, 17, 11,  3,  0,\n",
      "        2,  0,  0,  0,  9,  3,  6,  2,  8,  0, 19,  2, 14, 13,  9,  2, 15,\n",
      "        5,  2, 11, 12, 19,  9,  8, 19, 11,  6,  0,  2,  0,  3,  4, 13,  3,\n",
      "       14,  9,  8,  3,  2, 16, 15,  4,  5,  5, 11,  9,  5, 14,  0, 17,  4,\n",
      "       13,  0,  8,  8, 14, 17,  9,  3, 12,  9,  5,  9, 17,  3,  3, 12, 17,\n",
      "        8, 16,  0, 12,  5,  8,  8, 14, 12, 17,  3,  6, 15, 12, 17,  3, 12,\n",
      "        2, 15, 15, 15,  5, 16,  5,  8,  0,  5, 13, 13, 12,  0, 14,  8, 14,\n",
      "        9, 11,  4,  5, 13, 16,  5,  2,  0,  2, 15, 17, 12,  2, 12, 13, 12,\n",
      "        9,  5, 13, 12, 12,  0,  0, 12, 15,  5,  9,  5, 16, 11, 16, 10,  0,\n",
      "       16,  5, 15,  5,  0, 12, 10,  0,  2, 11, 11,  3,  5,  0,  2,  5, 17,\n",
      "        5, 11, 15, 15,  5, 11, 18,  6,  1,  2, 15, 16, 18, 10,  5,  2, 14,\n",
      "       17,  7, 16, 16, 15, 16, 14, 16, 18,  0,  9, 12, 16, 19, 11, 11,  6,\n",
      "        9, 19,  8, 13,  7, 15, 15, 13, 15,  5,  0, 15, 11,  2, 11,  6, 19,\n",
      "        4,  5, 19, 15, 16, 12, 18,  5, 19,  4,  2,  4, 11, 14,  4,  6,  1,\n",
      "        6,  4, 15, 12, 14,  2, 18, 13, 14,  9,  7, 11, 11, 11, 18,  5,  4,\n",
      "       14, 12,  8, 14,  9, 11,  4,  8,  9,  4, 11,  7, 13, 17,  8,  3, 17,\n",
      "       16, 13, 11,  2,  5, 16, 16, 16,  7,  0, 11, 11,  9, 16, 15, 16, 17,\n",
      "       13, 17,  4, 16,  2, 15,  3, 19, 13,  9, 12, 19, 17,  9,  5, 15,  0,\n",
      "        6, 13,  5,  1,  9, 12, 12,  4, 12,  0,  2, 17,  4, 10, 17, 12, 13,\n",
      "       19,  5, 19,  9, 16,  9, 11, 11,  5, 15, 13,  0, 17,  5, 14, 15, 15,\n",
      "        4, 19,  1,  9,  3, 19,  4, 12, 15, 13, 10,  9, 14, 16,  5, 11, 11,\n",
      "        4, 16,  4, 15, 19, 16,  4,  3,  2, 17, 12,  4,  6, 15, 15, 19,  0,\n",
      "        6, 15, 13, 15,  9,  2, 14,  9, 10, 11, 12,  9,  7,  2, 13, 19,  9,\n",
      "       19, 19,  9, 15, 14, 16, 11, 16, 12, 15,  5, 16, 16, 16, 13, 15, 14,\n",
      "        9, 13,  4, 15, 13,  0,  5,  0, 15,  2,  7, 14,  2, 13, 15, 14, 11,\n",
      "       18,  9, 12,  5, 12,  1, 19, 14, 13, 13, 14, 17, 15,  8, 16, 15,  0,\n",
      "        2, 11, 11, 11, 15,  3, 19, 15, 18, 16,  5,  0, 16,  8, 19,  6,  9,\n",
      "       11,  5, 14,  2, 15,  9, 17, 11, 12,  5, 12,  0, 10,  0, 15,  6,  8,\n",
      "        2,  2,  3,  3,  8,  4,  4, 12, 13, 15,  5, 17,  9,  7,  4,  5,  8,\n",
      "       13,  5, 15,  3,  8, 16, 11, 17,  2,  7,  3,  8, 17, 10,  7, 16,  2,\n",
      "        3,  3,  3,  7, 14, 16, 16, 11, 12, 17,  0, 16,  3, 13, 19,  5, 15,\n",
      "       17, 15, 16, 11,  9, 13, 14,  5, 11, 14, 13,  0,  0, 16,  0,  2, 17,\n",
      "       11, 16, 13,  5, 17,  9, 12,  5, 10, 17, 18, 13,  2, 14,  2, 17, 19,\n",
      "        9, 13,  5, 12,  7, 18,  0,  8,  7, 12,  6, 16,  2,  5,  6,  4,  6,\n",
      "       12, 15, 12,  9, 10,  5,  5,  4,  5,  9,  8,  6, 12, 12, 12, 13,  7,\n",
      "        9,  7,  8, 11, 16, 12, 17, 12,  0, 11, 12, 15, 16, 16,  4, 15,  0,\n",
      "        0,  8,  4,  0, 15,  4,  7, 16, 13, 19, 15, 16,  5, 13, 17, 15, 17,\n",
      "        3,  7,  3, 18,  3,  9, 13,  8,  3, 11, 15,  8, 14, 18, 11, 12,  3,\n",
      "        7, 13, 19, 16, 15, 11, 19, 11,  8, 15, 17, 11, 17,  2,  4, 16, 17,\n",
      "        2, 16, 11,  5, 17, 19, 15,  3, 12, 14, 12,  7,  5, 16, 14, 19,  9,\n",
      "       16, 14, 11,  9], dtype=int32)\n",
      "> \u001b[32m/tmp/ipykernel_960888/2807009226.py\u001b[39m(\u001b[92m54\u001b[39m)\u001b[36mstep\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Apply mutation\u001b[39;00m\n",
      "\u001b[32m     53\u001b[39m         new_state = self.state.copy()\n",
      "\u001b[32m---> 54\u001b[39m         new_state[pos] = aa_idx\n",
      "\u001b[32m     55\u001b[39m \n",
      "\u001b[32m     56\u001b[39m         \u001b[38;5;66;03m# Reward from fitness function\u001b[39;00m\n",
      "\n",
      "> \u001b[32m/tmp/ipykernel_960888/2807009226.py\u001b[39m(\u001b[92m57\u001b[39m)\u001b[36mstep\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     55\u001b[39m \n",
      "\u001b[32m     56\u001b[39m         \u001b[38;5;66;03m# Reward from fitness function\u001b[39;00m\n",
      "\u001b[32m---> 57\u001b[39m         reward = self.fitness_fn(self.idxs_to_letters(new_state), self.DMS)\n",
      "\u001b[32m     58\u001b[39m \n",
      "\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# You can choose episode termination rule:\u001b[39;00m\n",
      "\n",
      "(735,)\n",
      "\u001b[92m     52\u001b[39m         \u001b[38;5;66;03m# Apply mutation\u001b[39;00m\n",
      "\u001b[92m     53\u001b[39m         new_state = self.state.copy()\n",
      "\u001b[92m     54\u001b[39m         new_state[pos] = aa_idx\n",
      "\u001b[92m     55\u001b[39m \n",
      "\u001b[92m     56\u001b[39m         \u001b[38;5;66;03m# Reward from fitness function\u001b[39;00m\n",
      "\u001b[32m---> 57\u001b[39m         reward = self.fitness_fn(self.idxs_to_letters(new_state), self.DMS)\n",
      "\u001b[92m     58\u001b[39m \n",
      "\u001b[92m     59\u001b[39m         \u001b[38;5;66;03m# You can choose episode termination rule:\u001b[39;00m\n",
      "\u001b[92m     60\u001b[39m         \u001b[38;5;66;03m# e.g., fixed length episode of mutations\u001b[39;00m\n",
      "\u001b[92m     61\u001b[39m         terminated = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[92m     62\u001b[39m         truncated = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "KeyboardInterrupt\n",
      "> \u001b[32m/tmp/ipykernel_960888/2807009226.py\u001b[39m(\u001b[92m57\u001b[39m)\u001b[36mstep\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     55\u001b[39m \n",
      "\u001b[32m     56\u001b[39m         \u001b[38;5;66;03m# Reward from fitness function\u001b[39;00m\n",
      "\u001b[32m---> 57\u001b[39m         reward = self.fitness_fn(self.idxs_to_letters(new_state), self.DMS)\n",
      "\u001b[32m     58\u001b[39m \n",
      "\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# You can choose episode termination rule:\u001b[39;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('aav_wt.txt', 'r') as file:\n",
    "    wt = file.readline().strip()\n",
    "\n",
    "def make_env():\n",
    "    # Provide your own initial sequence + fitness_fn\n",
    "    return ProteinEnv(wt, fitness_ESM, 'aav_dms.csv')\n",
    "\n",
    "vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.99,\n",
    "    n_epochs=10,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    ")\n",
    "total_timesteps = 1\n",
    "tqdm_cb = TQDMCallback(total_timesteps=total_timesteps, algo='PPO')\n",
    "model.learn(total_timesteps=total_timesteps, callback=tqdm_cb)\n",
    "model.save(\"ppo_pretraining\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
